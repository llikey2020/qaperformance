stages:
  - prepare
  - deploy
  - deploy-charts
  - verify
  - run
  - collect
  - cleanup

variables:
  RUN_CONDITION:
    value: cold
    description: |
      (cold/warm) Whether to clear Alluxio cache or not before the run. 
      Manually starting the cleanup stage after the run will still clear Alluxio cache regardless
  JOB:
    value: JavaWordCount
    description: JavaWordCount, SampleResult, RunTPCDS, RunTPCHSmallDatePartition, RunTPCHLargeDatePartition
  DEPLOY_PROJECT: alluxio, history, mysql, sparkDashboard, schema, metadata
  SPARK_IMAGE_TAG:
    value: latest
    description: Spark container image tag to test
  SPARK_DRIVER_MEMORY:
    value: 4g
    description: spark.driver.memory
  SPARK_EXECUTOR_MEMORY:
    value: 8g
    description: spark.executor.memory
  SPARK_EXECUTOR_CORES:
    value: 1
    description: spark.executor.cores
  SPARK_EXECUTOR_REQUEST_CORES:
    value: 1000m
    description: spark.kubernetes.executor.request.cores
  SPARK_DYNAMIC_ALLOCATION_ENABLED:
    value: "false"
    description: spark.dynamicAllocation.enabled
  SPARK_DYNAMIC_ALLOCATION_MAX_EXECUTORS:
    value: 4
    description: spark.dynamicAllocation.maxExecutors
  SPARK_DYNAMIC_ALLOCATION_MIN_EXECUTORS:
    value: 2
    description: spark.dynamicAllocation.minExecutors
  CACHE_SSD_SIZE:
    value: 40G
    description: Cache SSD quota
  SPARK_WAREHOUSE:
    value: spark-warehouse/
    description: Path in Alluxio UFS for the Spark SQL warehouse files
  SPARK_SQL_PERF_JAR:
    value: spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar
    description: Path to package registry for the Spark SQL performance jar
  ALLUXIO_VERSION:
    value: 2.6.0
  ALLUXIO_SVC: alluxio-master-0.${KUBE_NAMESPACE}:19998
  SPARK_DEPENDENCY_DIR:
    value: spark-files/performance/
    description: Path in Alluxio UFS for the Spark SQL jar(s)
  SPARK_HISTORY_SERVER_ENABLED:
    value: "false"
    description: (true/false) Enable/disable spark history server
  SPARK_EVENT_LOG_ENABLED:
    value: "false"
    description: (true/false) Enable/disable spark event logging to persist Spark UI for the job
  SPARK_VERSION: 3.1.2-sdp-SNAPSHOT
  # Project variables configured in Settings -> CI/CD -> Variables:
  # - ALLUXIO_UFS
  # - AWS_ACCESS_KEY_ID
  # - AWS_SECRET_ACCESS_KEY

Prepare environment:
  rules:
    - if: '$ONLY_CLEANUP == "false"'
  image: ${CI_REGISTRY}/planetrover/infrastructure/kubectl
  stage: prepare
  script:
    - | 
      env | sort

      kubectl cluster-info

      kubectl delete pod ${SPARK_DRIVER_POD_NAME} --ignore-not-found=true

      if [ ${RUN_CONDITION} == "cold" ]
      then
        helm uninstall alluxio || true
      fi 

Verify docker image:
  rules:
    - if: '$JOB == "JavaWordCount" && $ONLY_CLEANUP == "false"'
  extends:
    - .spark-run
  stage: run
  script:
    - cd /opt/spark
    - |
      ./bin/spark-submit \
        --name "verification" \
        --deploy-mode cluster \
        --class org.apache.spark.examples.JavaWordCount \
        local:///opt/spark/examples/jars/spark-examples_2.12-${SPARK_VERSION}.jar \
        alluxio://${ALLUXIO_SVC}/matt/lorem.txt

Sample Performance Result:
  rules:
    - if: '$JOB == "SampleResult" && $ONLY_CLEANUP == "false"'
  extends:
    - .spark-run
  variables:
    GIT_STRATEGY: clone
  stage: run
  script:
    - cd /opt/spark
    - |
      ./bin/spark-submit \
        --name "sample" \
        --deploy-mode cluster \
        --class "SampleResult" \
        --conf spark.kubernetes.file.upload.path=alluxio://${ALLUXIO_SVC}/${SPARK_DEPENDENCY_DIR} \
        file://${CI_PROJECT_DIR}/perf-run-files/sampleresult_2.12-0.1.jar

TPCDS:
  rules:
    - if: '$JOB == "RunTPCDS" && $ONLY_CLEANUP == "false"'
  extends:
    - .spark-run
  stage: run
  script:
    - cd /opt/spark
    - |
      ./bin/spark-submit \
        --deploy-mode cluster \
        --name "tpcds" \
        --class com.databricks.spark.sql.perf.RunTPCDS \
        ${CI_API_V4_URL}/projects/13/packages/generic/spark-sql-perf/0.5.1/${SPARK_SQL_PERF_JAR}?job_token=${CI_JOB_TOKEN}

TPCH-small-partition:
  rules:
    - if: '$JOB == "RunTPCHSmallDatePartition" && $ONLY_CLEANUP == "false"'
  extends:
    - .spark-run
  stage: run
  script:
    - cd /opt/spark
    - |
      ./bin/spark-submit \
        --deploy-mode cluster \
        --name "tpch small partition" \
        --class com.databricks.spark.sql.perf.RunTPCHSmallDatePartition \
        ${CI_API_V4_URL}/projects/13/packages/generic/spark-sql-perf/0.5.1/${SPARK_SQL_PERF_JAR}?job_token=${CI_JOB_TOKEN}

TPCH-large-partition:
  rules:
    - if: '$JOB == "RunTPCHLargeDatePartition"'
  extends:
    - .spark-run
  stage: run
  script:
    - cd /opt/spark
    - |
      ./bin/spark-submit \
        --deploy-mode cluster \
        --name "tpch parge partition" \
        --class com.databricks.spark.sql.perf.RunTPCHLargeDatePartition \
        ${CI_API_V4_URL}/projects/13/packages/generic/spark-sql-perf/0.5.1/${SPARK_SQL_PERF_JAR}?job_token=${CI_JOB_TOKEN}

Collect logs:
  rules:
    - if: '$JOB != "JavaWordCount" && $ONLY_CLEANUP == "false"'
  image: ${CI_REGISTRY}/planetrover/infrastructure/kubectl
  stage: collect
  variables:
    SPARK_LOG: spark.log
    ALLUXIO_LOG: alluxio.log
    PARSED_RESULT: ${CI_JOB_STARTED_AT}_${RUN_CONDITION}_${CACHE_SSD_SIZE}.log
  script:
    - kubectl logs spark-driver | tee ${SPARK_LOG}
    - kubectl exec alluxio-master-0 -c alluxio-master -- alluxio fsadmin report metrics | tee ${ALLUXIO_LOG}
    - bash perf-run-files/perfLogParser.sh ${SPARK_LOG} ${ALLUXIO_LOG} | tee ${PARSED_RESULT}
  artifacts:
    paths:
      - ${PARSED_RESULT}